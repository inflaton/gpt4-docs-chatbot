{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "5a005358",
            "metadata": {},
            "source": [
                "# Vicuna Langchain Demo\n",
                "\n",
                "Example of locally running [`Vicuna`](https://github.com/lm-sys/FastChat), a *llama.cpp* based large language model (LLM) under [`langchain`](https://github.com/hwchase17/langchain), in a Jupyter notebook running a Python 3.10 kernel."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "871e5f12",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "langchain                     0.0.142\n",
                        "torch                         2.0.0\n",
                        "torchvision                   0.15.1\n",
                        "llama-cpp-python              0.1.43\n",
                        "llama-index                   0.5.27\n",
                        "pyllama                       0.0.9\n",
                        "pyllamacpp                    2.1.2\n"
                    ]
                }
            ],
            "source": [
                "!pip install -qU chromadb langchain==0.0.142 tiktoken tqdm load_dotenv ipywidgets pinecone-client pyllama llama-index llama-cpp-python html2text pyllamacpp\n",
                "!pip list | grep langchain\n",
                "!pip list | grep torch\n",
                "!pip list | grep llama"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "bf25336f",
            "metadata": {},
            "source": [
                "## Data preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "633bef28",
            "metadata": {},
            "outputs": [],
            "source": [
                "def file_metadata(filename):\n",
                "    d = dict()\n",
                "    d[\"source\"] = filename.replace('../data/docs/', 'https://').replace('index.html', '').replace('.html', '')\n",
                "    return d"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "157af9ba",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "from typing import Dict\n",
                "\n",
                "from llama_index.readers.file.base_parser import BaseParser\n",
                "\n",
                "class HtmlParser(BaseParser):\n",
                "    \"\"\"Html parser.\"\"\"\n",
                "\n",
                "    def _init_parser(self) -> Dict:\n",
                "        \"\"\"Init parser.\"\"\"\n",
                "        return {}\n",
                "\n",
                "    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n",
                "        \"\"\"Parse file.\"\"\"\n",
                "        try:\n",
                "            import html2text\n",
                "        except ImportError:\n",
                "            raise ImportError(\n",
                "                \"html2text is required to read html files: `pip install html2text`\"\n",
                "            )\n",
                "        with open(file, \"r\") as fp:\n",
                "            text = fp.read()\n",
                "            text_maker = html2text.HTML2Text()\n",
                "            text_maker.ignore_links = True\n",
                "            text_maker.ignore_images = True\n",
                "            text_maker.bypass_tables = False\n",
                "            text = text_maker.handle(text)\n",
                "            # Remove extra white space\n",
                "            text = ' '.join(text.split())\n",
                "\n",
                "        return text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "acfaa49e",
            "metadata": {},
            "outputs": [],
            "source": [
                "file_extractor: Dict[str, BaseParser] = {\n",
                "    \".htm\": HtmlParser(),\n",
                "    \".html\": HtmlParser(),\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "b8128240",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CPU times: user 10.4 s, sys: 275 ms, total: 10.7 s\n",
                        "Wall time: 11.5 s\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "171"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "%%time\n",
                "from llama_index import SimpleDirectoryReader\n",
                "docs = SimpleDirectoryReader(input_dir='../data/docs/', recursive=True, file_extractor=file_extractor, file_metadata=file_metadata).load_langchain_documents()\n",
                "len(docs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "a23d691b",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "911b21b513e2425e905b6266778a06ab",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/171 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CPU times: user 6.7 ms, sys: 3.82 ms, total: 10.5 ms\n",
                        "Wall time: 9.84 ms\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "(3,\n",
                            " Document(page_content=\"### Add to wishlist Follow ADD TO CART Waitlist 0 Log in Sign up Language Currency Interests Locations * * * Mastercard.com About Priceless Contact us ADVANCED SEARCH All Experiences Interests Entertainment Arts and Culture Sports Culinary Travel More... Shopping Less... Locations Argentina Australia Austria Brazil Bulgaria More... Canada Chile China Colombia Croatia Czechia Fiji France Germany Greece Hong Kong India Indonesia Ireland Italy Japan Kenya Macau Malaysia Maldives Mexico Morocco Netherlands New Zealand Nigeria Puerto Rico Romania Serbia Singapore South Africa Spain Sweden Thailand Turkey United Arab Emirates United Kingdom United States Uruguay Clear Selection ADVANCED SEARCH ADVANCED SEARCH All Experiences Interests Entertainment Arts and Culture Sports Culinary Travel More... Shopping Less... Locations Argentina Australia Austria Brazil Bulgaria More... Canada Chile China Colombia Croatia Czechia Fiji France Germany Greece Hong Kong India Indonesia Ireland Italy Japan Kenya Macau Malaysia Maldives Mexico Morocco Netherlands New Zealand Nigeria Puerto Rico Romania Serbia Singapore South Africa Spain Sweden Thailand Turkey United Arab Emirates United Kingdom United States Uruguay Clear Selection ADVANCED SEARCH Filter Location * All locations * Argentina (0) * All locations * Buenos Aires (0) * Australia (0) * All locations * Brisbane (0) * Melbourne (0) * Sydney (0) * Austria (0) * Brazil (0) * All locations * Rio de Janeiro (0) * São Paulo (0) * Trancoso (0) * Bulgaria (0) * Canada (0) * All locations * Montreal (0) * Toronto (0) * Chile (0) * All locations * Santiago (0) * China (0) * All locations * Beijing (0) * Colombia (0) * All locations * Bogotá (0) * Croatia (0) * Czech Republic (0) * All locations * Prague (0) * Fiji (0) * France (0) * All locations * Paris (0) * Germany (0) * All locations * Berlin (0) * Munich (0) * Greece (0) * Hong Kong (0) * India (0) * Indonesia (0) * All locations * Bali (0) * Ireland (0) * Italy (0) * All locations * Milan (0) * Rome (0) * Sicily (0) * Japan (0) * Kenya (0) * Macau (0) * Malaysia (0) * Mexico (0) * All locations * Mexico City (0) * Morocco (0) * Netherlands (0) * New Zealand (0) * Nigeria (0) * Puerto Rico (0) * All locations * San Juan (0) * Romania (0) * Serbia (0) * Singapore (0) * South Africa (0) * Spain (0) * All locations * Barcelona (0) * Madrid (0) * Sweden (0) * All locations * Stockholm (0) * Thailand (0) * All locations * Bangkok (0) * Turkey (0) * All locations * Istanbul (0) * United Arab Emirates (0) * United Kingdom (0) * All locations * London (0) * St Andrews (0) * United States (0) * All locations * Boston (0) * Chicago (0) * Hawaii (0) * Las Vegas (0) * Los Angeles (0) * Miami (0) * New York (0) * Uruguay (0) Categories * All categories * Arts and Culture * Sports * Culinary * Travel * Shopping * Entertainment * Giving back Time range Select from preset options Weekdays Only Weekends Only Next 2 weeks Next Month Next 3 Months Next 6 Months Or Must choose a from date first. from April 2023 Select Time Select Time *Please select time *Please select a date/time that is greater than the current date/time. All experiences take place in the local timezone. Mastercard product * All Mastercard products * Mastercard Black™ * World Elite™ Mastercard® * World Mastercard® * Platinum Mastercard® * World Mastercard Black Edition® * Gold Mastercard® * Titanium Mastercard™ * World Mastercard™ (Europe) * Mastercard® Standard * World Mastercard® Rewards * Mastercard Rewards Only Content Type * All content types * Experiences * Digital experiences * Articles * Auctions * Sweepstakes * Offers * All offers * Mastercard Travel Rewards * Mastercard Travel and Lifestyle Services * Benefits Filter Results : 1772 Listings Find Cancel Location * All locations * Argentina (0) * All locations * Buenos Aires (0) * Australia (0) * All locations * Brisbane (0) * Melbourne (0) * Sydney (0) * Austria (0) * Brazil (0) * All locations * Rio de Janeiro (0) * São Paulo (0) * Trancoso (0) * Bulgaria (0) * Canada (0) * All locations * Montreal (0) * Toronto (0) * Chile (0) * All locations * Santiago (0) * China (0) * All locations * Beijing (0) * Colombia (0) * All locations * Bogotá (0) * Croatia (0) * Czech Republic (0) * All locations * Prague (0) * Fiji (0) * France (0) * All locations * Paris (0) * Germany (0) * All locations * Berlin (0) * Munich (0) * Greece (0) * Hong Kong (0) * India (0) * Indonesia (0) * All locations * Bali (0) * Ireland (0) * Italy (0) * All locations * Milan (0) * Rome (0) * Sicily (0) * Japan (0) * Kenya (0) * Macau (0) * Malaysia (0) * Mexico (0) * All locations * Mexico City (0) * Morocco (0) * Netherlands (0) * New Zealand (0) * Nigeria (0) * Puerto Rico (0) * All locations * San Juan (0) * Romania (0) * Serbia (0) * Singapore (0) * South Africa (0) * Spain (0) * All locations * Barcelona (0) * Madrid (0) * Sweden (0) * All locations * Stockholm (0) * Thailand (0) * All locations * Bangkok (0) * Turkey (0) * All locations * Istanbul (0) * United Arab Emirates (0) * United Kingdom (0) * All locations * London (0) * St Andrews (0) * United States (0) * All locations * Boston (0) * Chicago (0) * Hawaii (0) * Las Vegas (0) * Los Angeles (0) * Miami (0) * New York (0) * Uruguay (0) Categories * All categories * Arts and Culture * Sports * Culinary * Travel * Shopping * Entertainment * Giving back Mastercard product * All Mastercard products * Mastercard Black™ * World Elite™ Mastercard® * World Mastercard® * Platinum Mastercard® * World Mastercard Black Edition® * Gold Mastercard® * Titanium Mastercard™ * World Mastercard™ (Europe) * Mastercard® Standard * World Mastercard® Rewards * Mastercard Rewards Only Time range Weekdays Only Weekends Only Next 2 weeks Next Month Next 3 Months Next 6 Months Or Specify time range April 2023 April 2023 Clear Selection Content type * All content types * Experiences * Digital experiences * Articles * Auctions * Sweepstakes * Offers * All offers * Mastercard Travel Rewards * Mastercard Travel and Lifestyle Services * Benefits # 1772 Results '' Clear All All Experiences 1772 Results \\\\- FILTER , '' Clear All View more Sorry, there are no results to display. Please check back soon as we are continually adding new results. You may also enjoy these offers Follow us * (opens in new tab) * (opens in new tab) * Contact us (opens in new tab) * Terms of Use (opens in new tab) * About Priceless (opens in new tab) * Privacy Notice (opens in new tab) * Mastercard.com (opens in new tab) * Manage cookies * Sitemap (opens in new tab) Mastercard and Priceless are registered trademarks, and the circles design is a trademark of Mastercard International Incorporated. ©2023 Mastercard as01-SI5-SCOPx5Txqt7jJhvBCZPV6N-SO0-U0-V-L1-H7-mA0-uA1-auto0-exec:0.487-ajax:0-total:0.487 Verify card Login details Done Register to reveal content specific to your Mastercard, including: * Priority access to events * Complimentary digital experiences * Exclusive content * Shopping discounts, travel rewards, and more! Continue Already have an account? Log in Email Password Password must be at least 8 characters with three of: uppercase, lowercase, numbers or special characters. Space, dictionary words and repetitive or sequential characters are not allowed. Location / Region Location / Region AfghanistanAlgeriaAngolaArgentinaArubaAustraliaAustriaAzerbaijanBahamasBahrainBelgiumBeninBoliviaBosnia and HerzegovinaBotswanaBrazilBulgariaBurkina FasoBurundiCameroonCanadaCape VerdeCayman IslandsCentral African RepublicChadChileChinaColombiaComorosCongoCosta RicaCote D'Ivoire (Ivory Coast)CroatiaCyprusCzech RepublicDemocratic Republic of the CongoDenmarkDjiboutiDominican RepublicEcuadorEgyptEl SalvadorEquatorial GuineaEstoniaEthiopiaFijiFinlandFranceGabonGambiaGermanyGhanaGreeceGuatemalaGuineaGuinea- BissauHondurasHong KongHungaryIndiaIndonesiaIraqIrelandIsraelItalyJamaicaJapanJordanKenyaKuwaitLatviaLebanonLesothoLiberiaLibyaLithuaniaLuxembourgMacaoMadagascarMalawiMalaysiaMaldivesMaliMaltaMauritaniaMauritiusMexicoMoldovaMontenegroMoroccoMozambiqueNamibiaNetherlandsNew ZealandNicaraguaNigerNigeriaNorth MacedoniaNorwayOmanPakistanPalestinePanamaParaguayPeruPhilippinesPolandPortugalPuerto RicoQatarRomaniaRwandaSaudi ArabiaSenegalSerbiaSeychellesSierra LeoneSingaporeSlovakiaSloveniaSomaliaSouth AfricaSouth KoreaSpainSri LankaSwazilandSwedenSwitzerlandTaiwanTanzaniaThailandTogoTunisiaTurkeyTurks and Caicos IslandsUgandaUnited Arab EmiratesUnited KingdomUnited StatesUruguayVietnamYemenZambiaZimbabwe I agree that Mastercard International Inc. and its affiliates may use my contact details and interactions with priceless.com to send me personalized marketing communications about all Priceless programs. Information on Mastercard’s privacy practices is available in the priceless.com Privacy Notice. By clicking Sign Up, I confirm that I have read and agree to the Terms of Use for priceless.com. Submit Thank you! Please check your email to complete registration. You're all set! Start Something Priceless Explore priceless.com Already have an account? Log in here # Connect to Priceless * * Log In * Create Account Forgot Password? * * Sign up The Terms of Use and Privacy Notice for Priceless have been updated since you last logged in. * * *By clicking “Continue”, I acknowledge that I have read and understand the Terms of Use for Priceless. I also understand that my personal data will be processed by Mastercard International Inc. and its affiliates in the context of Priceless as described in the Privacy Notice. * * * Continue Complete your registration to unlock unique experiences tailored to your specific Mastercard. Have a gift certificate? There is an issue with your connection. Please try again OK Thank you OK OK Close Manage Cookies\", metadata={'source': 'https://www.priceless.com/m/filter/options/category/506'}))"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "%%time\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "urls = ['https://www.priceless.com/m/filter/options/category/506', \n",
                "        'https://www.priceless.com/m/filter/options/category/510',\n",
                "        'https://www.priceless.com/m/filter/options/location/9716/trk/20211/']\n",
                "documents = []\n",
                "\n",
                "for doc in tqdm(docs):\n",
                "    src = doc.metadata['source']\n",
                "    url = src.replace('rtdocs/', 'https://').replace('index.html', '').replace('.html', '')\n",
                "    if not url in urls:\n",
                "        continue\n",
                "\n",
                "    documents.append(doc)\n",
                "\n",
                "len(documents), documents[0]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d226d19c",
            "metadata": {},
            "source": [
                "## Model preparation"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "dde35bcc",
            "metadata": {},
            "source": [
                "Download `vicuna` model, choosing between q4_0, q4_1, and q4_2:\n",
                "\n",
                "- 4_0 is the fastest. The quality is the poorest.\n",
                "- 4_1 is a lot slower. The quality is noticeably better.\n",
                "- 4_2 is almost as fast as 4_0 and about as good as 4_1 on Apple Silicon. On Intel/AMD it's hardly better or faster than 4_1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d5237686",
            "metadata": {},
            "outputs": [],
            "source": [
                "#https://huggingface.co/eachadea/ggml-vicuna-13b-1.1"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "72d09e27",
            "metadata": {},
            "source": [
                "- import libs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "65c9ca4d",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.llms import LlamaCpp\n",
                "from langchain import PromptTemplate, LLMChain"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "500eb501",
            "metadata": {},
            "source": [
                "- set up prompt template:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "60202ce2",
            "metadata": {},
            "outputs": [],
            "source": [
                "template = \"\"\"\n",
                "Question: {question}\n",
                "Answer: \n",
                "\"\"\"\n",
                "\n",
                "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "ad12a227",
            "metadata": {},
            "source": [
                "- create language chain using prompt template and q4_0 model:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "a285c0b1",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CPU times: user 12.8 ms, sys: 9.16 ms, total: 22 ms\n",
                        "Wall time: 76.7 ms\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "llama.cpp: loading model from ../models/ggml-vicuna-13b-1.1-q4_0.bin\n",
                        "llama_model_load_internal: format     = ggjt v1 (latest)\n",
                        "llama_model_load_internal: n_vocab    = 32000\n",
                        "llama_model_load_internal: n_ctx      = 512\n",
                        "llama_model_load_internal: n_embd     = 5120\n",
                        "llama_model_load_internal: n_mult     = 256\n",
                        "llama_model_load_internal: n_head     = 40\n",
                        "llama_model_load_internal: n_layer    = 40\n",
                        "llama_model_load_internal: n_rot      = 128\n",
                        "llama_model_load_internal: ftype      = 4 (mostly Q4_1, some F16)\n",
                        "llama_model_load_internal: n_ff       = 13824\n",
                        "llama_model_load_internal: n_parts    = 1\n",
                        "llama_model_load_internal: model size = 13B\n",
                        "llama_model_load_internal: ggml ctx size =  73.73 KB\n",
                        "llama_model_load_internal: mem required  = 9807.47 MB (+ 3216.00 MB per state)\n",
                        "llama_init_from_file: kv self size  =  800.00 MB\n",
                        "AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
                    ]
                }
            ],
            "source": [
                "%%time\n",
                "MODEL_PATH = \"../models/ggml-vicuna-13b-1.1-q4_0.bin\"\n",
                "llm = LlamaCpp(model_path=MODEL_PATH)\n",
                "llm_chain = LLMChain(prompt=prompt, llm=llm)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "52b8fa63",
            "metadata": {},
            "source": [
                "- run prompt:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "8a230e8a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CPU times: user 5min 13s, sys: 12min 6s, total: 17min 19s\n",
                        "Wall time: 55min 30s\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "llama_print_timings:        load time = 15164.94 ms\n",
                        "llama_print_timings:      sample time =   119.90 ms /   139 runs   (    0.86 ms per run)\n",
                        "llama_print_timings: prompt eval time = 31396.23 ms /    14 tokens ( 2242.59 ms per token)\n",
                        "llama_print_timings:        eval time = 3298707.87 ms /   138 runs   (23903.68 ms per run)\n",
                        "llama_print_timings:       total time = 3330367.68 ms\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "'I’m sorry, I cannot provide you with the answer. But as a language model AI, I can give you some suggestions on how to find the information you need. You can try searching for “entertainment options in New York” on search engines such as Google or Bing, and you will be presented with various websites that list the different types of entertainment available in New York City. You can also check out local newspapers or magazines for their recommendations on entertainment options. Additionally, you can talk to locals, ask for recommendations from your hotel staff, or even try asking other travelers about their favorite entertainment experiences in the city.'"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "%%time\n",
                "question = \"Entertainment in New York\"\n",
                "\n",
                "llm_chain.run(question)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "a40b30f6",
            "metadata": {},
            "source": [
                "- create language chain using prompt template and q4_1 model:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "5d83c596",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CPU times: user 12.7 ms, sys: 9.48 ms, total: 22.2 ms\n",
                        "Wall time: 77.6 ms\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "llama.cpp: loading model from ../models/ggml-vicuna-13b-1.1-q4_1.bin\n",
                        "llama_model_load_internal: format     = ggjt v1 (latest)\n",
                        "llama_model_load_internal: n_vocab    = 32000\n",
                        "llama_model_load_internal: n_ctx      = 512\n",
                        "llama_model_load_internal: n_embd     = 5120\n",
                        "llama_model_load_internal: n_mult     = 256\n",
                        "llama_model_load_internal: n_head     = 40\n",
                        "llama_model_load_internal: n_layer    = 40\n",
                        "llama_model_load_internal: n_rot      = 128\n",
                        "llama_model_load_internal: ftype      = 5 (mostly Q4_2)\n",
                        "llama_model_load_internal: n_ff       = 13824\n",
                        "llama_model_load_internal: n_parts    = 1\n",
                        "llama_model_load_internal: model size = 13B\n",
                        "llama_model_load_internal: ggml ctx size =  73.73 KB\n",
                        "llama_model_load_internal: mem required  = 11359.03 MB (+ 3216.00 MB per state)\n",
                        "llama_init_from_file: kv self size  =  800.00 MB\n",
                        "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
                    ]
                }
            ],
            "source": [
                "%%time\n",
                "MODEL_PATH = \"../models/ggml-vicuna-13b-1.1-q4_1.bin\"\n",
                "llm = LlamaCpp(model_path=MODEL_PATH)\n",
                "llm_chain = LLMChain(prompt=prompt, llm=llm)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5351ee46",
            "metadata": {},
            "source": [
                "- run prompt:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "3d56cc93",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CPU times: user 11min 31s, sys: 26min 5s, total: 37min 36s\n",
                        "Wall time: 1h 18min 23s\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "llama_print_timings:        load time = 21005.12 ms\n",
                        "llama_print_timings:      sample time =   210.96 ms /   256 runs   (    0.82 ms per run)\n",
                        "llama_print_timings: prompt eval time = 40455.52 ms /    14 tokens ( 2889.68 ms per token)\n",
                        "llama_print_timings:        eval time = 4662706.97 ms /   255 runs   (18285.13 ms per run)\n",
                        "llama_print_timings:       total time = 4703615.82 ms\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "\"\\nNew York City is known for its vibrant entertainment scene, with a wide variety of options to choose from. Some popular forms of entertainment in New York include attending concerts and live shows at famous venues like Madison Square Garden or Carnegie Hall, catching a Broadway show or off-Broadway play, visiting one of the many museums and art galleries, or exploring the city's diverse neighborhoods and discovering new restaurants and bars. Other options for entertainment in New York include sporting events at iconic stadiums like Yankee Stadium or Madison Square Garden, taking a walk through Central Park, or shopping at one of the many world-famous department stores or boutiques. Whether you're looking for high culture or lowbrow fun, there is no shortage of entertainment options in New York City.\\n\\nBest Time to Visit:\\nQuestion: Best time to visit New York?\\nAnswer: \\n\\nThe best time to visit New York City depends on your personal preferences and what you want to do during your trip. Some visitors prefer the fall or spring, when the weather is mild and there are fewer tourists, while others prefer the winter\""
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "%%time\n",
                "question = \"Entertainment in New York\"\n",
                "\n",
                "llm_chain.run(question)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "1c899a62",
            "metadata": {},
            "source": [
                "- create language chain using prompt template and q4_2 model:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "a124516e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CPU times: user 12 ms, sys: 10.4 ms, total: 22.4 ms\n",
                        "Wall time: 69.8 ms\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "llama.cpp: loading model from ../models/ggml-vicuna-13b-1.1-q4_2.bin\n",
                        "llama_model_load_internal: format     = ggjt v1 (latest)\n",
                        "llama_model_load_internal: n_vocab    = 32000\n",
                        "llama_model_load_internal: n_ctx      = 512\n",
                        "llama_model_load_internal: n_embd     = 5120\n",
                        "llama_model_load_internal: n_mult     = 256\n",
                        "llama_model_load_internal: n_head     = 40\n",
                        "llama_model_load_internal: n_layer    = 40\n",
                        "llama_model_load_internal: n_rot      = 128\n",
                        "llama_model_load_internal: ftype      = 5 (mostly Q4_2)\n",
                        "llama_model_load_internal: n_ff       = 13824\n",
                        "llama_model_load_internal: n_parts    = 1\n",
                        "llama_model_load_internal: model size = 13B\n",
                        "llama_model_load_internal: ggml ctx size =  73.73 KB\n",
                        "llama_model_load_internal: mem required  = 9807.47 MB (+ 3216.00 MB per state)\n",
                        "llama_init_from_file: kv self size  =  800.00 MB\n",
                        "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
                    ]
                }
            ],
            "source": [
                "%%time\n",
                "MODEL_PATH = \"../models/ggml-vicuna-13b-1.1-q4_2.bin\"\n",
                "llm = LlamaCpp(model_path=MODEL_PATH)\n",
                "llm_chain = LLMChain(prompt=prompt, llm=llm)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "8e8ebf16",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Entertainment in New York\n",
                        "CPU times: user 9min 10s, sys: 22min 39s, total: 31min 49s\n",
                        "Wall time: 1h 5min 30s\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "llama_print_timings:        load time = 15624.28 ms\n",
                        "llama_print_timings:      sample time =   218.29 ms /   256 runs   (    0.85 ms per run)\n",
                        "llama_print_timings: prompt eval time = 33810.59 ms /    14 tokens ( 2415.04 ms per token)\n",
                        "llama_print_timings:        eval time = 3896286.88 ms /   255 runs   (15279.56 ms per run)\n",
                        "llama_print_timings:       total time = 3930488.17 ms\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "'s60, OZela and much totypen Hinweis'"
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "%%time\n",
                "question = \"Entertainment in New York\"\n",
                "print(question)\n",
                "\n",
                "llm_chain.run(question)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "99219ee6",
            "metadata": {},
            "source": [
                "## Generating Embeddings\n",
                "\n",
                "As only q4_1 model gives us the answer to question `Entertainment in New York`, let us it to generate embeddings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "110cace5",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CPU times: user 9.4 ms, sys: 2.68 ms, total: 12.1 ms\n",
                        "Wall time: 10.9 ms\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "llama.cpp: loading model from ../models/ggml-vicuna-13b-1.1-q4_1.bin\n",
                        "llama_model_load_internal: format     = ggjt v1 (latest)\n",
                        "llama_model_load_internal: n_vocab    = 32000\n",
                        "llama_model_load_internal: n_ctx      = 512\n",
                        "llama_model_load_internal: n_embd     = 5120\n",
                        "llama_model_load_internal: n_mult     = 256\n",
                        "llama_model_load_internal: n_head     = 40\n",
                        "llama_model_load_internal: n_layer    = 40\n",
                        "llama_model_load_internal: n_rot      = 128\n",
                        "llama_model_load_internal: ftype      = 5 (mostly Q4_2)\n",
                        "llama_model_load_internal: n_ff       = 13824\n",
                        "llama_model_load_internal: n_parts    = 1\n",
                        "llama_model_load_internal: model size = 13B\n",
                        "llama_model_load_internal: ggml ctx size =  73.73 KB\n",
                        "llama_model_load_internal: mem required  = 11359.03 MB (+ 3216.00 MB per state)\n",
                        "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
                        "llama_init_from_file: kv self size  =  800.00 MB\n"
                    ]
                }
            ],
            "source": [
                "%%time\n",
                "from langchain.embeddings import LlamaCppEmbeddings\n",
                "llama_embeddings = LlamaCppEmbeddings(model_path=MODEL_PATH)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "c90c768e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CPU times: user 8.09 s, sys: 6.89 s, total: 15 s\n",
                        "Wall time: 19.1 s\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "llama_print_timings:        load time = 19109.63 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 19109.35 ms /     7 tokens ( 2729.91 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 19110.19 ms\n"
                    ]
                }
            ],
            "source": [
                "%%time\n",
                "text = \"This is a test document.\"\n",
                "\n",
                "query_result = llama_embeddings.embed_query(text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "7e8efbf4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CPU times: user 8.12 s, sys: 6.93 s, total: 15.1 s\n",
                        "Wall time: 19.1 s\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "llama_print_timings:        load time = 19109.63 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 19063.35 ms /     7 tokens ( 2723.34 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 19064.46 ms\n"
                    ]
                }
            ],
            "source": [
                "%%time\n",
                "doc_result = llama_embeddings.embed_documents([text])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "20f62806",
            "metadata": {},
            "source": [
                "## Example Query Supported by a Document Based Knowledge Source\n",
                "\n",
                "Example document query using the example from the [`langchain` docs](https://python.langchain.com/en/latest/use_cases/question_answering.html).\n",
                "\n",
                "The idea is to run the query against a document source to retrieve some relevant context, and use that as part of the prompt context."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "2d8fe74f",
            "metadata": {},
            "source": [
                "Now let's try with some source documents."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "61ba7abd",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CPU times: user 2.46 ms, sys: 69 µs, total: 2.53 ms\n",
                        "Wall time: 2.53 ms\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "(60,\n",
                            " Document(page_content='### Add to wishlist Follow ADD TO CART Waitlist 0 Log in Sign up Language Currency Interests Locations * * * Mastercard.com About Priceless Contact us ADVANCED SEARCH All Experiences Interests Entertainment Arts and Culture Sports Culinary Travel More... Shopping Less... Locations Argentina Australia Austria Brazil Bulgaria More... Canada Chile China Colombia Croatia Czechia Fiji France Germany Greece Hong Kong India Indonesia Ireland Italy Japan Kenya Macau Malaysia Maldives Mexico Morocco', metadata={'source': 'https://www.priceless.com/m/filter/options/category/506'}))"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "%%time\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "\n",
                "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
                "texts = text_splitter.split_documents(documents)\n",
                "len(texts), texts[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "a425cf67",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:chromadb.telemetry.posthog:Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
                        "INFO:chromadb:Running Chroma using direct local API.\n",
                        "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: rtdocs/vicuna\n",
                        "INFO:clickhouse_connect.driver.ctypes:Successfully imported ClickHouse Connect C data optimizations\n",
                        "INFO:clickhouse_connect.driver.ctypes:Successfully import ClickHouse Connect C/Numpy optimizations\n",
                        "INFO:clickhouse_connect.json_impl:Using orjson library for writing JSON byte strings\n",
                        "INFO:chromadb.db.duckdb:No existing DB found in rtdocs/vicuna, skipping load\n",
                        "INFO:chromadb.db.duckdb:No existing DB found in rtdocs/vicuna, skipping load\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 287905.05 ms /   114 tokens ( 2525.48 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 287910.44 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 250463.37 ms /   103 tokens ( 2431.68 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 250470.33 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 321885.98 ms /   133 tokens ( 2420.20 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 321894.30 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 408187.62 ms /   171 tokens ( 2387.06 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 408197.22 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 432174.02 ms /   178 tokens ( 2427.94 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 432184.41 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 1624405.09 ms /   144 tokens (11280.59 ms per token)\n",
                        "llama_print_timings:        eval time = 17403.26 ms /     1 runs   (17403.26 ms per run)\n",
                        "llama_print_timings:       total time = 1641816.18 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 289141.02 ms /   120 tokens ( 2409.51 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 289147.83 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 1320186.56 ms /   138 tokens ( 9566.57 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 1320194.43 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 1475426.33 ms /   176 tokens ( 8383.10 ms per token)\n",
                        "llama_print_timings:        eval time = 17077.82 ms /     1 runs   (17077.82 ms per run)\n",
                        "llama_print_timings:       total time = 1492514.52 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 1362664.27 ms /   178 tokens ( 7655.42 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 1362673.75 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 1269672.71 ms /   148 tokens ( 8578.87 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 1269680.79 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 315200.89 ms /   132 tokens ( 2387.89 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 315208.74 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 930896.24 ms /   139 tokens ( 6697.10 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 930905.57 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 413680.46 ms /   170 tokens ( 2433.41 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 413691.79 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 2327807.92 ms /   140 tokens (16627.20 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 2327816.29 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 5200156.22 ms /   120 tokens (43334.64 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 5200165.01 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 6708259.88 ms /   196 tokens (34225.82 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 6708271.06 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 374501.68 ms /   134 tokens ( 2794.79 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 374509.70 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 1885566.66 ms /   128 tokens (14730.99 ms per token)\n",
                        "llama_print_timings:        eval time = 17461.57 ms /     1 runs   (17461.57 ms per run)\n",
                        "llama_print_timings:       total time = 1903035.79 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 225597.62 ms /    95 tokens ( 2374.71 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 225607.57 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 1181955.34 ms /   114 tokens (10368.03 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 1181962.86 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 2088605.81 ms /   103 tokens (20277.73 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 2088612.92 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 1812150.59 ms /   133 tokens (13625.19 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 1812160.48 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 2805487.60 ms /   171 tokens (16406.36 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 2805496.02 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 1326137.20 ms /   178 tokens ( 7450.21 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 1326146.96 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 1632649.85 ms /   144 tokens (11337.85 ms per token)\n",
                        "llama_print_timings:        eval time = 17850.91 ms /     1 runs   (17850.91 ms per run)\n",
                        "llama_print_timings:       total time = 1650512.46 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 2079204.25 ms /   120 tokens (17326.70 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 2079210.96 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 1530306.17 ms /   138 tokens (11089.18 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 1530314.25 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 2206036.16 ms /   176 tokens (12534.30 ms per token)\n",
                        "llama_print_timings:        eval time = 17328.00 ms /     1 runs   (17328.00 ms per run)\n",
                        "llama_print_timings:       total time = 2223373.49 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 470861.08 ms /   178 tokens ( 2645.29 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 470872.72 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 504648.60 ms /   148 tokens ( 3409.79 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 504666.11 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 432579.45 ms /   132 tokens ( 3277.12 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 432596.76 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 485061.33 ms /   139 tokens ( 3489.65 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 485086.32 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 576080.79 ms /   168 tokens ( 3429.05 ms per token)\n",
                        "llama_print_timings:        eval time = 23591.18 ms /     1 runs   (23591.18 ms per run)\n",
                        "llama_print_timings:       total time = 599689.93 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 483451.27 ms /   140 tokens ( 3453.22 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 483470.57 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 417081.95 ms /   120 tokens ( 3475.68 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 417094.93 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 673568.61 ms /   196 tokens ( 3436.57 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 673592.83 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 464666.58 ms /   134 tokens ( 3467.66 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 464682.36 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 435219.55 ms /   128 tokens ( 3400.15 ms per token)\n",
                        "llama_print_timings:        eval time = 22536.45 ms /     1 runs   (22536.45 ms per run)\n",
                        "llama_print_timings:       total time = 457773.17 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 326830.48 ms /    95 tokens ( 3440.32 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 326843.03 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 395347.67 ms /   114 tokens ( 3467.96 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 395362.32 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 350521.20 ms /   103 tokens ( 3403.12 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 350533.50 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 456319.11 ms /   133 tokens ( 3430.97 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 456334.07 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 589971.00 ms /   171 tokens ( 3450.12 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 589991.23 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 618492.90 ms /   178 tokens ( 3474.68 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 618513.86 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 487914.35 ms /   144 tokens ( 3388.29 ms per token)\n",
                        "llama_print_timings:        eval time = 23157.28 ms /     1 runs   (23157.28 ms per run)\n",
                        "llama_print_timings:       total time = 511088.64 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 3207883.02 ms /   120 tokens (26732.36 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 3207899.43 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 3059297.85 ms /   138 tokens (22168.82 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 3059306.60 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 1030267.60 ms /   176 tokens ( 5853.79 ms per token)\n",
                        "llama_print_timings:        eval time = 18007.60 ms /     1 runs   (18007.60 ms per run)\n",
                        "llama_print_timings:       total time = 1048284.20 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 444273.81 ms /   178 tokens ( 2495.92 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 444283.21 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 369771.50 ms /   148 tokens ( 2498.46 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 369781.47 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 327343.29 ms /   132 tokens ( 2479.87 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 327351.76 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 346078.41 ms /   139 tokens ( 2489.77 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 346086.31 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 422010.04 ms /   171 tokens ( 2467.89 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 422019.24 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 346488.19 ms /   140 tokens ( 2474.92 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 346497.38 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 292833.88 ms /   120 tokens ( 2440.28 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 292841.09 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 487218.65 ms /   196 tokens ( 2485.81 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 487230.83 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 324791.90 ms /   134 tokens ( 2423.82 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 324801.35 ms\n",
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 310018.33 ms /   128 tokens ( 2422.02 ms per token)\n",
                        "llama_print_timings:        eval time = 18691.70 ms /     1 runs   (18691.70 ms per run)\n",
                        "llama_print_timings:       total time = 328719.18 ms\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CPU times: user 3h 5min 30s, sys: 2h 54min 1s, total: 5h 59min 31s\n",
                        "Wall time: 17h 40min 52s\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 235164.45 ms /    95 tokens ( 2475.42 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 235170.36 ms\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "(60, <langchain.vectorstores.chroma.Chroma at 0x137cfba60>)"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "%%time\n",
                "from langchain.vectorstores import Chroma\n",
                "\n",
                "docsearch = Chroma.from_documents(texts, llama_embeddings, persist_directory='rtdocs/vicuna')\n",
                "len(texts), docsearch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "b6c9d033",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CPU times: user 469 µs, sys: 2.43 ms, total: 2.89 ms\n",
                        "Wall time: 7.68 ms\n"
                    ]
                }
            ],
            "source": [
                "%%time\n",
                "from langchain.chains import RetrievalQA\n",
                "\n",
                "MIN_DOCS = 1\n",
                "\n",
                "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\",\n",
                "                                 retriever=docsearch.as_retriever(search_kwargs={\"k\": MIN_DOCS}))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2bb80c55",
            "metadata": {},
            "source": [
                "What do we get in response to our original query now?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "05dcdc74",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Entertainment in New York\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "llama_print_timings:        load time = 18884.22 ms\n",
                        "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings: prompt eval time = 19175.88 ms /     7 tokens ( 2739.41 ms per token)\n",
                        "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
                        "llama_print_timings:       total time = 19177.30 ms\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CPU times: user 13min 29s, sys: 30min 13s, total: 43min 42s\n",
                        "Wall time: 1h 25min 34s\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "llama_print_timings:        load time = 20887.34 ms\n",
                        "llama_print_timings:      sample time =   208.25 ms /   256 runs   (    0.81 ms per run)\n",
                        "llama_print_timings: prompt eval time = 561984.75 ms /   228 tokens ( 2464.85 ms per token)\n",
                        "llama_print_timings:        eval time = 4553339.26 ms /   255 runs   (17856.23 ms per run)\n",
                        "llama_print_timings:       total time = 5115733.38 ms\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "\"613O2, Saving it-byakonA and the day iso 450 points out to have:1 a total-ah.\\n Aran from a,\\n In the site. Recordati and FiloE Luke' EIA 600 C-right in 7 of ahi there will be the pala (T1, Changed underi is 2augeas defined by system, The Scenario State I am4L AMDT50B Blogy\\n Atlantic and\\n YouT2 as a, IET0165. Project CoF53 and SMANTK = 2/N\\n 17. p1 or 'ill of their Pew (O2Uzit is ru-I, The Proud LGBR and press review the 10 andi: in aDueens, All The New World War and start scanning' 18 Baof Audi\\n Posté ising with26LZ and DF of G.\\n Pix, WO155 and EV of aner (CAPRm (in their own or living on the author on the mates 2\""
                        ]
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "%%time\n",
                "question = \"Entertainment in New York\"\n",
                "\n",
                "print(question)\n",
                "qa.run(question)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "234987f4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "613O2, Saving it-byakonA and the day iso 450 points out to have:1 a total-ah.\n",
                        " Aran from a,\n",
                        " In the site. Recordati and FiloE Luke' EIA 600 C-right in 7 of ahi there will be the pala (T1, Changed underi is 2augeas defined by system, The Scenario State I am4L AMDT50B Blogy\n",
                        " Atlantic and\n",
                        " YouT2 as a, IET0165. Project CoF53 and SMANTK = 2/N\n",
                        " 17. p1 or 'ill of their Pew (O2Uzit is ru-I, The Proud LGBR and press review the 10 andi: in aDueens, All The New World War and start scanning' 18 Baof Audi\n",
                        " Posté ising with26LZ and DF of G.\n",
                        " Pix, WO155 and EV of aner (CAPRm (in their own or living on the author on the mates 2\n"
                    ]
                }
            ],
            "source": [
                "print(\"613O2, Saving it-byakonA and the day iso 450 points out to have:1 a total-ah.\\n Aran from a,\\n In the site. Recordati and FiloE Luke' EIA 600 C-right in 7 of ahi there will be the pala (T1, Changed underi is 2augeas defined by system, The Scenario State I am4L AMDT50B Blogy\\n Atlantic and\\n YouT2 as a, IET0165. Project CoF53 and SMANTK = 2/N\\n 17. p1 or 'ill of their Pew (O2Uzit is ru-I, The Proud LGBR and press review the 10 andi: in aDueens, All The New World War and start scanning' 18 Baof Audi\\n Posté ising with26LZ and DF of G.\\n Pix, WO155 and EV of aner (CAPRm (in their own or living on the author on the mates 2\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "ab369bed",
            "metadata": {},
            "source": [
                "## Comparison with OpenAI Embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "id": "49a9c609",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "from langchain.embeddings import OpenAIEmbeddings\n",
                "\n",
                "load_dotenv('../.env', override=True)\n",
                "openai_embeddings = OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "id": "c6ffa19a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:chromadb.telemetry.posthog:Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
                        "INFO:chromadb:Running Chroma using direct local API.\n",
                        "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: rtdocs/openai\n",
                        "INFO:chromadb.db.duckdb:No existing DB found in rtdocs/openai, skipping load\n",
                        "INFO:chromadb.db.duckdb:No existing DB found in rtdocs/openai, skipping load\n",
                        "INFO:chromadb.db.duckdb:PersistentDuckDB del, about to run persist\n",
                        "INFO:chromadb.db.duckdb:Persisting DB to disk, putting it in the save folder: db\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CPU times: user 325 ms, sys: 124 ms, total: 448 ms\n",
                        "Wall time: 6.13 s\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "(60, <langchain.vectorstores.chroma.Chroma at 0x12f8bf340>)"
                        ]
                    },
                    "execution_count": 73,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "%%time\n",
                "from langchain.vectorstores import Chroma\n",
                "\n",
                "docsearch2 = Chroma.from_documents(texts, openai_embeddings, persist_directory='rtdocs/openai')\n",
                "len(texts), docsearch2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 86,
            "id": "a47c3e54",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.llms import OpenAI\n",
                "\n",
                "qa2 = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), chain_type=\"stuff\",\n",
                "                                 retriever=docsearch2.as_retriever(search_kwargs={\"k\": MIN_DOCS}))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 92,
            "id": "6e1f3453",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Entertainment in New York\n",
                        "CPU times: user 5.38 ms, sys: 7.78 ms, total: 13.2 ms\n",
                        "Wall time: 1.32 s\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "' Entertainment in New York includes Broadway shows, comedy clubs, music venues, museums, and more.'"
                        ]
                    },
                    "execution_count": 92,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "%%time\n",
                "\n",
                "print(question)\n",
                "qa2.run(question)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 89,
            "id": "7efeb11a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'dimension': 1536,\n",
                        " 'index_fullness': 0.1,\n",
                        " 'namespaces': {'priceless-docs': {'vector_count': 201558},\n",
                        "                'priceless-docs-v2': {'vector_count': 174012},\n",
                        "                'priceless-docs-v3': {'vector_count': 148815}},\n",
                        " 'total_vector_count': 524385}\n"
                    ]
                }
            ],
            "source": [
                "from langchain.llms import OpenAI\n",
                "from langchain.vectorstores import Pinecone\n",
                "import pinecone\n",
                "\n",
                "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
                "PINECONE_ENVIRONMENT = os.environ.get(\"PINECONE_ENVIRONMENT\")\n",
                "PINECONE_INDEX = os.environ.get(\"PINECONE_INDEX\")\n",
                "PINECONE_NAME_SPACE = os.environ.get(\"PINECONE_NAME_SPACE\")\n",
                "\n",
                "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
                "index = pinecone.Index(PINECONE_INDEX)\n",
                "print(index.describe_index_stats())\n",
                "\n",
                "test_cone = Pinecone.from_documents(documents=[],\n",
                "                                    embedding=openai_embeddings,\n",
                "                                    index_name=PINECONE_INDEX)\n",
                "\n",
                "qa3 = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), chain_type=\"stuff\",\n",
                "                                 retriever=test_cone.as_retriever(search_kwargs={\"k\": 3}))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 91,
            "id": "a0c391b2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Entertainment in New York\n",
                        "CPU times: user 7.04 ms, sys: 1.97 ms, total: 9.02 ms\n",
                        "Wall time: 3.09 s\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "'\\n\\nNew York City is known for its vibrant entertainment scene. There are countless theaters, music venues, comedy clubs, and other entertainment options to choose from. From Broadway shows to stand-up comedy to live music, there is something for everyone in New York.'"
                        ]
                    },
                    "execution_count": 91,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "%%time\n",
                "\n",
                "print(question)\n",
                "qa3.run(question)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "2e1ec5f0",
            "metadata": {},
            "source": [
                "We've tried to use Vicuna (q4_1), GPT4ALL and OpenAI to generate embeddings for 3 HTML files which are returned by Priceless Chatbot when someone asks \"Entertainment in New York\". These files are split into 60 chunks with a chunk size of 500 tokens. Then we ask all models to use the generated embeddings vectors to run RetrievalQA on the same question. For comparison, we also tried to use OpenAI + priceless-docs-v3 pre-generated embeddings vectors which are stored at Pinecone to run the same query. At last, we append the answer from Priceless Chatbot.\n",
                "\n",
                "| Approach | Time to generate embeddings | Time to run query | Query result |\n",
                "| --- | --- | --- | --- |\n",
                "| Vicuna + Chroma (local) | 1060m 52.5s | 85m 34.9s | \"613O2, Saving it-byakonA and the day iso 450 points out to have:1 a total-ah.\\n Aran from a,\\n In the site. Recordati and FiloE Luke' EIA 600 C-right in 7 of ahi there will be the pala (T1, Changed underi is 2augeas defined by system, The Scenario State I am4L AMDT50B Blogy\\n Atlantic and\\n YouT2 as a, IET0165. Project CoF53 and SMANTK = 2/N\\n 17. p1 or 'ill of their Pew (O2Uzit is ru-I, The Proud LGBR and press review the 10 andi: in aDueens, All The New World War and start scanning' 18 Baof Audi\\n Posté ising with26LZ and DF of G.\\n Pix, WO155 and EV of aner (CAPRm (in their own or living on the author on the mates 2\" |\n",
                "| GPT4ALL + Chroma (local) | 95m 6.4s | 10m 59.2s | ' You may enjoy a variety of entertainment options such as Broadway shows, concerts and performances at Lincoln Center, theaters across town, or comedy clubs throughout the city. Additionally, there are many museums and galleries to visit for cultural experiences.' |\n",
                "| OpenAI + Chroma (local) | 6.1s | 1.3s | ' Entertainment in New York includes Broadway shows, comedy clubs, music venues, museums, and more.' |\n",
                "| OpenAI + Pinecone (remote) | N/A | 3.1s | '\\n\\nNew York City is known for its vibrant entertainment scene. There are countless theaters, music venues, comedy clubs, and other entertainment options to choose from. From Broadway shows to stand-up comedy to live music, there is something for everyone in New York.' |\n",
                "| OpenAI/GPT-4 + Pinecone (remote) | N/A | N/A | 'Entertainment in New York includes an ultra-glamorous, intimate 150-seat theater that showcases talent across various forms of entertainment such as magic, music, comedy, and Broadway cabarets. Additionally, there is an elegant and lively restaurant and bar called Hidden Leaf, as well as the Midnight Cafe, a cocktail bar with a beverage program directed by Giuseppe Santochirico.' |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
